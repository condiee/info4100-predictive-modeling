---
title: 'Group Project: Early Alert with LMS Data'
author: '[[Jacob Bee Ho Brown (jdb393), Emma Condie (erc97), Ellie Garell (emg223), Sanithia Edwards (se273), Yue Ji (yj359), Julia Lehman (jsl357), Heitung Sun (hs835)]]'
subtitle: INFO 4100 Learning Analytics
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

```{r warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
# This loads 3 datasets: cl=clickstream, a=assessment grades; m=module states.
load("info4100_edx_2020-10-17.rda")
```

# Introduction

**Goals:** The goal of this project is to learn how to work with raw Learning Management System (LMS) data and apply some of the prediction skills you have learned so far. You will develop a one-day early warning system for students who miss a graded submission. I am sharing with you an export of the class's edX log data thus far. I have anonymized the dataset and performed minimal data cleaning, leaving plenty of real-world messiness for you to tackle here. As always, you should start by getting to know the datasets. In this case, you should be able to really understand what is going on because it is YOUR data. In fact, you can navigate to the relevant pages on edX to see what page/action the data refers to.

**Group Project:** This is a group project and I expect you to work as a team to come up with the best possible prediction accuracy. Your team will submit one common solution (note that EACH team member will need to submit the knitted Word doc on edx to get credit like with the first group project). 

**Try Your Best:** All members of the TWO teams that achieve the highest F1 scores will receive an extra credit point, and their solutions will be featured. To be eligible, your prediction problem needs to be set up correctly (i.e. everything else needs to be correct).

# Step 1: Understand the data

There are three datasets which can be connected using the hash_id column (a hashed version of the user id) and I am giving you links to the official documentation which you should read to understand the data better:

1. Clickstream data (1 row per student per action): [click for documentation](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/tracking_logs.html#tracking-logs)
2. Module States (1 row per student per accessed content): original name [courseware-studentmodule (click for doumentation)](https://edx.readthedocs.io/projects/devdata/en/stable/internal_data_formats/sql_schema.html#courseware-studentmodule)
3. Assessment grades (1 row per assessment per student)

I have already converted date-time objects into a numeric `timestamp` for you.

To look up what pages URLs refer to (works for browser events, not server events), you can paste the URL into your browser. This should work for most URLs. I recommend doing this to be able to engineer more meaningful features.

*Question 1:* In the space below, explore each dataset using `head()`, `n_distinct(data$some_id)`, `summary()`, `table(data$column)`. You can also plot the distribution of variables with histograms or boxplots. Check out the data documentation linked above to understand the meaning of each column.

```{r eval=F}
############################################### 
###### BEGIN INPUT: Explore each dataset ###### 
###############################################

# Exploring Clickstreams
head(cl)
n_distinct(cl$hash_id)
n_distinct(cl$survey_id)
n_distinct(cl$time)
n_distinct(cl$name)
n_distinct(cl$event_type)
n_distinct(cl$referer)
n_distinct(cl$page)
n_distinct(cl$event_source)
n_distinct(cl$event)
n_distinct(cl$timestamp)
#summary(cl)
table(cl$hash_id)
#table(cl$survey_id)
#table(cl$time)
table(cl$name)
table(cl$event_type)
table(cl$referer)
table(cl$page)
table(cl$event_source)
cl$event
table(cl$timestamp)

# Exploring Module States
head(m)
n_distinct(m$hash_id)
n_distinct(m$module_type)
n_distinct(m$grade)
n_distinct(m$created)
n_distinct(m$modified)
n_distinct(m$max_grade)
n_distinct(m$module_id)
n_distinct(m$created_timestamp)
n_distinct(m$modified_timestamp)
#summary(m)
#table(cl$hash_id)
table(m$module_type)
table(m$grade)
table(m$created)
table(m$modified)
table(m$max_grade)
table(m$module_id)
table(m$created_timestamp)
table(m$modified_timestamp)

# Exploring Assessment grades
# add code here
head(a)
n_distinct(a$hash_id)
n_distinct(a$usage_key)
n_distinct(a$earned_graded)
n_distinct(a$first_attempted)
n_distinct(a$created)
n_distinct(a$created_timestamp)
n_distinct(a$modified_timestamp)
n_distinct(a$first_attempted_timestamp)
#summary(a)
#table(a$hash_id)
#table(a$usage_key)
table(a$earned_graded)
table(a$first_attempted)
table(a$created)
table(a$created_timestamp)
table(a$modified_timestamp)
table(a$first_attempted_timestamp)

###############################################
###############################################
```

You may notice that it would be helpful to combine the information about grades and time of first attempt with the module state data. Below I make this join for you. See that only 'sequential' modules have grade data associated with them. The boxplot shows when the different sequentials (containing problems) were attempted. This gives you an idea of the order of problems in the course.

```{r}
ma = m %>% left_join(
    a %>% select(hash_id:possible_graded, first_attempted_timestamp), 
    by = c("hash_id"="hash_id", "module_id"="usage_key")
)

# Only sequential modules have a grade associated with them
table(ma$module_type, ma$first_attempted_timestamp>0)

# We see that assignments were due (submitted) at different times
boxplot(ma$first_attempted_timestamp ~ ma$module_id)
```

# Step 2: Define a prediction task

Recall the guidelines for defining a good prediction problem covered in the Handbook chapter on prediction. You are looking for something actionable (an opportunity to intervene) and a situation that repeats (so the prediction can be useful in the future). The tradeoff with the dataset you have here is that on the one hand it is very relevant to you but on the other hand it is relatively small. Still, the data is fine-grained and sufficiently messy to give you a taste of LMS data analysis.

The prediction problem for this project is to build a one-day early warning system for missing a graded submission. Specifically, **your goal is to predict one day before the submission deadline, if a student will forget to submit an assignment**, so that the system can send a reminder. As you may have noticed during the data exploration phase above (if not, you should go back and examine this), there are several graded submissions and some students missed one or more of them. We define **missing a submission** as having an NA for `first_attempted_timestamp` but of course only for those that are past due.

### Instructions

1. Treat each graded assignment as a prediction task (thus there are x*n prediction opportunities where x = number of graded assignments and n = 31 students).
2. Create a dataset that has 1 row per student per graded assessment with the binary outcome (did they MISS it? yes/no) and several predictors (see next tip)
3. Predictors (i.e. features) need to be engineered with data from **24hrs before each assignment is due**, which of course varies across assignments; that means you have much more information to predict later assignments than earlier ones
4. Once your dataset is ready, split it into a training and a test set
5. Train a prediction model on the training data; you can try out any of the ones we have covered in the prediction homework and Random Forest
6. Keep tuning your model choice, model parameters (if any), and feature engineering
6. Finally, test your prediction accuracy on the test set

# Step 3: Getting you started

## Create the outcome variable

**Identify the graded assessments and whether a student did NOT submit**. Recall we want to have a *warning* system, so the outcome should be the negative action.

Get the outcome for each graded assignment. Figure out the deadline for each and compute the timestamp for 24hrs prior to the deadline. You probably want to use the `ma` dataset I created for you above.

`r boxplot(ma$first_attempted_timestamp ~ ma$module_id)`

The following table helps you see the various graded assignments to consider. We keep only those where possible_graded > 0. **I define the deadline as the 90th percentile of submissions (you may use this simplification).**

```{r}
ma %>% 
    filter(possible_graded > 0) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = mean(is.na(first_attempted_timestamp))
    ) %>% 
    arrange(deadline)

ma %>% 
    filter((possible_graded > 0) &(!is.na(first_attempted_timestamp))) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = sum(first_attempted_timestamp<deadline-60 * 60 * 24)/n()
    ) %>% 
    arrange(deadline)



```

Now you know which assessments (module_ids) to target. **Be sure to kick out the one with p_unsubmitted > 0.5**; They were not due yet when the export was created.

*Question 2:* Now build a dataset with an indicator for each person and each of these module_ids with 1=unsubmitted, 0=submitted. Keep track of the deadline: you only want to use features based on data up to 24hrs before it (i.e. `24 * 60 * 60` seconds).

```{r}
############################################### 
####### BEGIN INPUT: Define outcome ###########
###############################################

# edit ma to include a column with the module deadline
# and a binary column for the probability for whether the student submits on time
ma_edited = ma %>% 
    filter(possible_graded > 0) %>%
    group_by(module_id) %>% 
    summarise(
        deadline = quantile(first_attempted_timestamp, probs = .9, na.rm=T),
        p_unsubmitted = mean(is.na(first_attempted_timestamp))
    ) %>% 
    arrange(deadline) %>% 
    filter(p_unsubmitted <=0.5)
print(ma_edited)

# combine these new columns with ma 
ma_combined = ma %>% left_join(
    ma_edited %>% select(module_id:deadline,p_unsubmitted), 
    by = c("module_id"="module_id")
)

# create outcome dataset
outcome = ma_combined %>% 
    filter((possible_graded > 0)&(p_unsubmitted <=0.5)) %>%
    group_by(hash_id, module_id) %>% 
    summarise(
        unsubmitted = (is.na(first_attempted_timestamp))*1)

# add deadline column from ma_edited
outcome = outcome %>% left_join(
    ma_edited %>% select(module_id:deadline), 
    by = c("module_id"="module_id")
  )
outcome
############################################### 
############################################### 
```

## Feature Engineering

**For each graded assessment, identify what data is appropriate for feature engineering**

Before you start feature engineering, you need to constrain the data for **each** assessment.

Remember that the dataset we are aiming for has 1 row per person and assessment with several feature variables and one outcome variable. You created the outcome above. Now you need to create the appropriate features to join. I'm giving you an example for using `deadline = 1600304996` and creating 2 basic features from the clickstream. You should try to create a lot more features, including complex ones, that can use the clickstream or other datasets (but remember the timing constraint).

```{r}
# PROF CODE EXAMPLE FEATURE
secs_day = 60 * 60 * 24
example_deadline = 1600304996

example_features = cl %>% 
    filter(timestamp < example_deadline - secs_day) %>%
    group_by(hash_id) %>%
    summarise(
        num_events = n(),
        num_seq_goto = sum(event_type=="seq_goto")
    )

head(example_features)
```


*Question 3:* Engineer features for each student and assessment, subject to the timing constraint.

```{r}
############################################### 
###### BEGIN INPUT: Engineer features #########
###############################################

# want dataset with 1 row per person and assessment, with several feature variables, and 1 outcome variable
features = outcome
ma_features = data.frame()
cl_features = data.frame()
# for each graded assessment, select data to use for feature eng based on module ddl-1 day
for (row in 1:nrow(ma_edited)) #nrow(ma_edited)==13
{
  module_ddl = as.numeric(ma_edited[row,"deadline"])
  moduleid = as.character(ma_edited[row,"module_id"])
  ### modules that are available before ddl + 1 day, including the current module
  #filter out assignments that weren't due yet and data from the window 24hours before the deadline
  example_features = ma_combined %>% 
    #filter((possible_graded >0 ) & (deadline <= module_ddl) & (first_attempted_timestamp < module_ddl - secs_day)) %>%
    filter((possible_graded >0 ) & (deadline <= module_ddl - secs_day)) %>%
    group_by(hash_id) %>%
    summarise(
        #add our features for assessments here!
        #num_assignments = n(),
        diff = mean(deadline + secs_day - first_attempted_timestamp),
        #miss_ddl = sum(is.na(first_attempted_timestamp)),
        miss_ddl = sum(deadline - first_attempted_timestamp<0),
        #avg_miss_ddl = sum(deadline - first_attempted_timestamp<0)/n(),
        avg_grade = sum(earned_graded)/sum(possible_graded),
        #perf_score = sum(earned_graded==possible_graded),
        avg_perf_score = sum(earned_graded==possible_graded)/n(),
        #grade = earned_graded/possible_graded,
        #time_between_modification = mean(modified_timestamp-created_timestamp),
        # grade_above_avg = (earned_graded/possible_graded) > mean(earned_graded)/possible_graded,  # earned_graded/possible_graded) == mean(earned_graded)/possible_graded
        #more_than_1_possible_point = sum(possible_graded>1.0),
        #prev_attemp_last_hr = sum(first_attempted_timestamp > deadline - 3600), # not sure about this one
        #submitted_latest = !is.na(first_attempted_timestamp)
    )
  #feature checking if students submitted aldready the current assignment one day before the calculated deadline
  
  problem = ma_combined %>% 
    filter((module_type=="problem") & (modified_timestamp <= module_ddl - secs_day))%>%
    group_by(hash_id) %>%
    summarise(
      #full_grade = sum(grade==max_grade),
      #time_spent = mean(modified_timestamp - created_timestamp)
    )
  video_info = ma_combined %>% 
    filter((module_type=="video") & (modified_timestamp  <= (module_ddl - secs_day))) %>%
    group_by(hash_id) %>%
    summarise(
        #num_videos = n(),
        #time_on_videos = sum(modified_timestamp-created_timestamp),
        #time_per_video_avg = (sum(modified_timestamp-created_timestamp))/n()
    )
  #video_info
  features_per_module = features[c(-3,-4)] %>% filter(module_id == moduleid) %>% left_join(
    example_features, 
    by = c("hash_id"="hash_id")
  )
  # features_per_module = features_per_module %>% left_join(
  #   video_info, 
  #   by = c("hash_id"="hash_id")
  # )
  # features_per_module = features_per_module %>% left_join(
  #   problem, 
  #   by = c("hash_id"="hash_id")
  # )
  
  submitted = ma_combined %>% 
    filter((possible_graded >0 ) & (module_id == moduleid)) %>%
    group_by(hash_id) %>%
    summarise(
        #add our features for assessments here!
        #num_assignments = n(),
        #diff = mean(deadline + secs_day - first_attempted_timestamp),
        not_submitted_by_ddl = (!(first_attempted_timestamp < module_ddl - secs_day))*1,
        #miss_ddl = sum(deadline - first_attempted_timestamp<0),
        #avg_miss_ddl = sum(deadline - first_attempted_timestamp<0)/n(),
        #avg_grade = sum(earned_graded)/sum(possible_graded),
        #perf_score = sum(earned_graded==possible_graded),
        #avg_perf_score = sum(earned_graded==possible_graded)/n(),
        #grade = earned_graded/possible_graded,
        #time_between_modification = mean(modified_timestamp-created_timestamp),
        # grade_above_avg = (earned_graded/possible_graded) > mean(earned_graded)/possible_graded,  # earned_graded/possible_graded) == mean(earned_graded)/possible_graded
        #more_than_1_possible_point = sum(possible_graded>1.0),
       #  prev_attemp_last_hr = sum(first_attempted_timestamp > deadline - 3600), # not sure about this one
    )
  print(submitted)
  features_per_module = features_per_module %>% left_join(
    submitted,
    by = c("hash_id"="hash_id")
  )
  
  ma_features = rbind(ma_features, features_per_module)
  
  
  cl_features_before_module_ddl = cl %>% 
    filter(timestamp < module_ddl - secs_day) %>%
    group_by(hash_id) %>%
    summarise(
      # add our features for clickstream data here!
        num_events = log2(n()),
        #num_seq_goto = sum(event_type=="seq_goto"),
        #num_module = sum(str_detect(page,moduleid)),
        #pause_play_video = sum(event_type=="play_video"),
        #num_pause_video = sum(event_type=="pause_video"),
        num_video_interactions = sum(str_detect(event_type, "video")),
        #num_prob_check = sum(event_type=="problem_check"),
        #num_show_ans = sum(event_type=="showanswer"),
        num_links_clicked = sum(name=="edx.ui.lms.link_clicked"),
        #num_ref_from_mail = sum(str_detect(referer, "mail")),
        #time_spent = mean(diff(timestamp)),
        num_events_past_week = sum(timestamp > module_ddl - secs_day*7),
        num_events_past_three_day= sum(timestamp > module_ddl - secs_day*4),
        num_events_past_day = sum(timestamp > module_ddl - secs_day*2),
        # add more event types (follow URL to figure out what they are)
        # use page
        # use name
    )
  #print(cl_features_before_module_ddl)
  cl_features_per_module = features[c(-3,-4)] %>% filter(module_id == moduleid) %>% left_join(
    cl_features_before_module_ddl, 
    by = c("hash_id"="hash_id")
  )
  #print(features_per_module)
  cl_features = rbind(cl_features, cl_features_per_module)
}  

#print(cl_features)

# how do we save each set of features for each module into one dataset? 
#print(ma_features)
all_features = left_join(ma_features, cl_features, by = c("hash_id"="hash_id", "module_id"="module_id"))
all_features

###############################################
###############################################
```

# Step 4: Split your dataset

*Question 4:* We would like train the model on earlier assessments in order to make early alert predictions for later ones. As the hold-out test set, designate the four (4) last assessments (i.e. with the 4 latest computed deadlines, or the last 4 periods; same thing). You will use all the remaining data to train. Note that this may not be the best setup for all applications (e.g. if we wanted to use the model at the start of the course next year, but it is a reasonable approach if we wanted to use the model for the rest of this course offering). Identify the module_ids of the last four assignments, put data associated with their periods in the `test` dataset. Take all the remaining data (earlier periods excl the last 4) and put it in the `train` dataset.

```{r}
############################################### 
######## BEGIN INPUT: Split dataset ###########
###############################################

# Identify last 4 periods for testing
# add code here
all_features = left_join(ma_features, cl_features, by = c("hash_id"="hash_id", "module_id"="module_id"))
all_features = left_join(all_features, outcome, by = c("hash_id"="hash_id", "module_id"="module_id"))
sum(is.na(all_features[["unsubmitted"]])) #==0
sum(is.na(all_features[["miss_ddl"]])) # 96
sum(is.na(all_features[["num_events"]])) # 11
sum(all_features[["unsubmitted"]]==1) #63
#replace_na(1) will result in classification tree not working
for (col in colnames(all_features)){
  #if (col=="miss_ddl" || col=="unsubmitted"){
  #  all_features[[col]] = all_features[[col]] %>% replace_na(1)
  #}
  #else{
    all_features[[col]] = all_features[[col]] %>% replace_na(0)
  #}
}
#all_features =  all_features %>% drop_na()
#all_features_dropped_na
#all_features = all_features %>% dplyr::mutate(x = replace_na(diff, 0))
#$diff = all_features$diff %>% replace_na(0)
#all_features$avg_perf_score = all_features$avg_perf_score %>% replace_na(0)
#all_features$num_events = all_features$num_events %>% replace_na(0)
all_features
last_four = ma_edited$module_id[10:13]
#print(last_four)

modelingTime = 1602288653 + 1000000		
K = 1000000
T = 1/2
#all_features$weight = K*exp(-(modelingTime - all_features$deadline)/T)
all_features$weight = 1/((modelingTime - all_features$deadline)/K)
# Split the dataset into train and test based on the module_ids or periods
 test = all_features %>% filter(module_id %in% last_four)
 print(test)
 train = all_features %>% filter(!module_id %in% c(last_four,"block-v1:Cornellx+INFO4100+Fall2020+type@sequential+block@bc80acbecf2d43f7b1d24704ff03fdf3"))
 print(train)

 
summary(all_features)
cor(all_features[! (names(all_features) %in% c("hash_id","module_id","deadline"))])[,"unsubmitted"]
###############################################
###############################################
```

# Step 5: Train your models

*Question 5:* Train a prediction model and iterate on it. You should try out different algorithms that you have learned so far. You can go back and check your features and refine them to get better performance. To check how well you are doing, you should focus on your training data and compute the F1 score: `F1 = 2/[(1/recall)+(1/precision)]`. Report your F1 score on the training data below (don't forget this!).

```{r}
############################################### 
####### BEGIN INPUT: Train and report #########
###############################################

# Fit  model to training data
# add code here
### logistic regression
m_logreg = glm(as.factor(train$unsubmitted) ~ . - hash_id - module_id - deadline - weight, data = train, family = "binomial")
# the output are the coefficients:
m_logreg


### knn
library(class)
#predictors = c("avg_perf_score","diff","num_events")
#m_knn = knn(train = train[,predictors], test = train[,predictors], cl = train$unsubmitted, k = 3)
m_knn = knn(train[,3:(ncol(train)-3)], train[,3:(ncol(train)-3)], train$unsubmitted, k = 2)
# the output are the predictions:
#m_knn

### classification tree
library(rpart)
# = rpart(unsubmitted ~ avg_grade + more_than_1_possible_point + num_events + num_video_interactions + num_events_past_week + num_events_past_three_day + num_events_past_day, data = train, method = "class")
m_class_tree = rpart(unsubmitted ~ diff+ miss_ddl + avg_grade + avg_perf_score +num_events+num_video_interactions+  not_submitted_by_ddl+ num_links_clicked + num_events_past_week+ num_events_past_three_day + num_events_past_day , data = train, method = "class")
#m_class_tree = rpart(unsubmitted ~ diff + miss_ddl + avg_grade + num_events + num_events_past_week, data = train, method = "class")
#m_class_tree = rpart(unsubmitted ~ avg_perf_score + diff + num_events, data = train, method = "class")
#m_class_tree = rpart(unsubmitted ~ . - hash_id - module_id - deadline - weight, data = train, method = "class")
# the output are the decision trees
#m_class_tree

# you can even plot it!
plot(m_class_tree, uniform = T)
text(m_class_tree, use.n = F, all = TRUE, cex = .8)

# prune the trees to avoid overfitting by limiting tree complexity
#cp_class_tree = m_class_tree$cptable[which.min(m_class_tree$cptable[,"xerror"]),"CP"]
#m_class_tree_pruned = prune(m_class_tree, cp = cp_class_tree)


### naive bayes

library(e1071)
#m_nb = naiveBayes(unsubmitted ~ diff + avg_perf_score + num_events, data = train)
m_nb = naiveBayes(as.factor(unsubmitted) ~ . - hash_id - module_id - deadline - weight, data = train, method = "class")
# the output are a-prior and conditional probabilities
#m_nb

# Make predictions on the test dataset
# logreg: this returns the probability of dropout, so you can set Prob > 0.5 to mean Dropout
p_logreg = predict(m_logreg, newdata = train, type = "response") > 0.5
#p_logreg
#
# knn: this already has the prediction
p_knn = m_knn
# class tree
p_class_tree = predict(m_class_tree, newdata = train, type = "class")
# naive bayes
p_nb = predict(m_nb, newdata = train, type = "class")

cm_logreg = table(true = train$unsubmitted, predicted = p_logreg)
cm_logreg
cm_knn = table(true = train$unsubmitted, predicted = p_knn)
cm_class_tree = table(true = train$unsubmitted, predicted = p_class_tree)
cm_nb = table(true = train$unsubmitted, predicted = p_nb)

# convenience function for evaluation of confusion matrix
cm_eval = function(cm) {
    list(
        accur = sum(diag(cm)) / sum(cm),
        recall = cm[2,2] / sum(cm[2,]),
        precision = cm[2,2] / sum(cm[,2]),
        F1 = 2 / (1/(cm[2,2] / sum(cm[2,])) + 1/(cm[2,2] / sum(cm[,2])))
    )
}

cm_eval(cm_logreg)
cm_eval(cm_knn)
cm_eval(cm_class_tree)
cm_eval(cm_nb)
```


```{r}

library(caret)

#all_features$weight
fitControl <- trainControl(method = "none")

#set.seed(825)
knnFit4 <- train(as.factor(unsubmitted) ~ . , data = train[,!colnames(train) %in% c("hash_id","module_id","deadline","weight")], 
                 method = "knn", 
                 trControl = fitControl, 
                 weights = train$weight
                 )
knnFit4


#predict(gbmFit4, newdata = as.factor(head(test)))
p_knn = predict(knnFit4, newdata = train)
cm_knn = table(true = train$unsubmitted, predicted = p_knn)
cm_eval(cm_knn)
p_knn = predict(knnFit4, newdata = test)
cm_knn = table(true = test$unsubmitted, predicted = p_knn)
cm_eval(cm_knn)
###############################################
###############################################
```

# Step 6: Test your model

*Question 6:* Using the model that you arrived at, predict on the held-out test data and report your final F1 score. Typically, you would only do this once at the very end, but for this project it is actually rather hard to do well on the test set, so you can try your model (sparingly to avoid overfitting too much) on the test data to compute the testing F1 score.

```{r}
############################################### 
####### BEGIN INPUT: Test and report ##########
###############################################

# Make predictions on the test dataset
# logreg: this returns the probability of dropout, so you can set Prob > 0.5 to mean Dropout
#m_logreg = glm(unsubmitted ~ . - hash_id - module_id - deadline, data = train, family = "binomial")
#p_logreg = predict(m_logreg, newdata = test, type = "response") > 0.5
m_logreg = glm(unsubmitted ~ . , data = train[,!colnames(train) %in% c("hash_id","module_id","deadline","weight")], family = "binomial")
m_logreg$xlevels[["module_id"]] = union(m_logreg$xlevels[["module_id"]], levels(test$module_id))
p_logreg = predict(m_logreg, newdata = test, type = "response") > 0.5

#
# knn: this already has the prediction
p_knn = knn(train[,3:(ncol(train)-3)], test[,3:(ncol(train)-3)], train$unsubmitted, k = 5)
# class tree
p_class_tree = predict(m_class_tree, newdata = test, type = "class")
# naive bayes
p_nb = predict(m_nb, newdata = test, type = "class")
# add code here

# here is the confusion matrix for the logreg model:
cm_logreg = table(true = test$unsubmitted, predicted = p_logreg)
cm_knn = table(true = test$unsubmitted, predicted = p_knn)
cm_class_tree = table(true = test$unsubmitted, predicted = p_class_tree)

cm_nb = table(true = test$unsubmitted, predicted = p_nb)

cm_logreg
cm_eval(cm_logreg)
cm_knn
cm_eval(cm_knn)
cm_class_tree
cm_eval(cm_class_tree)
cm_nb 
cm_eval(cm_nb)

###############################################
###############################################
```

# Step 7: Report

*Question 7:* As a team, write a brief report. Imagine your supervisor asked you to investigate the possibility of an early warning system. She would like to know what model to use, what features are important, and most importantly how well it would work. Given what you've learned, would you recommend implementing the system? Write your report answering the above questions here:

%######## BEGIN INPUT: Summarize findings ############

In question 2, we created the outcome indicator based on is.na(first_attempted_timestamp), that is, if the students actually submitted the homework before the actual deadline or not since the edX system does not accept submissions after the deadline. There are 13 assignments that are used for this analysis. There are two types of assignments: 9 of which are reading comprehension, reading reflection, and video reflection, which all have a possible grade of 1, and 4 of which are the individual and group homework with possible grades varying from 8 to 19 points. 

After testing many different factors, we included the ones with the highest correlation values in our model. Despite these features being the most important to our model compared to the many others that we engineered, they still had low correlation values, the highest only about 0.2. We learned that predicting which students will not submit an assignment through this system is difficult because overall, the possible factors have low correlation values, resulting in F1 values below 0.4 for the models. This is true even for the logistic regression model, our best performing model with an F1 score of 0.3783784.

Given our results, we would in fact not recommend implementing the system. Our most important features included the average difference in time between when a student submitted and the calculated deadline, the number of clickstream events they generated, their average grade, how often on average they got a perfect score on an assignment, and whether or not they submitted assignments on time in the past.

One possible reason that our system did not work well might be the NA values in the original datasets and in the features we engineered. The NA values in  first_attempted_timestamp were used to determine the outcome variable. In some of our features, we calculated the difference between the deadline and first_attempted_timestamp, which sometimes resulted in NA values for those features. Some models, such as the Naive Bayes, are able to still make predictions even when missing attributes are present. Thus, the impact of missing data largely relates to the choice of learning algorithm. We attempted to work around this issue by manually changing the null values to 0 where we thought it made sense to do so, but this altered the fit of the models to the data. We also considered replacing the missing values with a “normalized” value (i.e. mean of known values), but this would not have worked for binary features where we were seeing the NA values.

In evaluating the obstacles we faced when creating this model, we feel we had a particularly difficult time relating the data back to the corresponding due dates. Particularly because we had no basis on which events were related to which assignment, we were making these classifications purely based on due date (which was an assumed value) and the timestamp of the event. While these methods were useful, future models may be more successful with more clear data relating to the due date and the relevant events to each assignment. 

While creating the models, we made some other observations we believe may be helpful if someone were to look into designing a similar system in the future. When we generated the percentage of unsubmitted assignments for each module we noticed one assignment, the only video reflection assignment, has the highest percentage of being unsubmitted relative to other assignments. Upon examining it more closely, we concluded that this might be because this assignment was due on a Friday followed by the first group project, an unusual time that students might forget. It is possible that our model is less effective because unique circumstances such as this may be influencing why a student doesn’t submit more often, as opposed to other more predictable situations such as a student consistently struggling in the course.

%###############################################

# Submit Project

This is the end of the project. Please **Knit a Word doc report** that shows both the R code and R output and upload it on the EdX platform. EACH TEAM MEMBER NEEDS TO SUBMIT THE REPORT ON EDX TO GET CREDIT.
